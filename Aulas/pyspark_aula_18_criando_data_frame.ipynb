{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55642970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"pyspark_aula_18_criando_data_frame\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0da3a753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-5GAGVV6:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark_aula_12</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x21f7939b380>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac47b0eb",
   "metadata": {},
   "source": [
    "# Dicionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67768fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'nome': 'João', 'idade': 25},\n",
       " {'nome': 'Maria', 'idade': 30},\n",
       " {'nome': 'Pedro', 'idade': 35}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados = [\n",
    "    {\"nome\": \"João\", \"idade\": 25},\n",
    "    {\"nome\": \"Maria\", \"idade\": 30},\n",
    "    {\"nome\": \"Pedro\", \"idade\": 35}\n",
    "]\n",
    "\n",
    "dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e907519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Converta para Pandas DataFrame primeiro\n",
    "pandas_df = pd.DataFrame(dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec006a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00482d07",
   "metadata": {},
   "source": [
    "# Criando Tuplas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b3b1315",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados2 = [\n",
    "    (\"João\", 25),\n",
    "    (\"Maria\", 30),\n",
    "    (\"Pedro\", 26)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48eb1760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('João', 25), ('Maria', 30), ('Pedro', 26)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados\n",
    "dados2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c80979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "spark.createDataFrame(dados2, [\"nome\", \"idade\"])\n",
    "\n",
    "#Ta dando erro no momento do createDataFrame, então eu estou convertendo para o pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "013db873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nome</th>\n",
       "      <th>idade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>João</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maria</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pedro</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nome  idade\n",
       "0  João     25\n",
       "1  Maria     30\n",
       "2  Pedro     26"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame(dados2, columns=[\"nome\", \"idade\"])\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45fc7b3",
   "metadata": {},
   "source": [
    "# Schema com tipo de dado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ac3b191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('nome', StringType(), True), StructField('idade', IntegerType(), True)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.types as T\n",
    "\n",
    "schema = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"nome\", StringType(), True),\n",
    "        T.StructField(\"idade\", IntegerType(), True),\n",
    "    ]\n",
    ")\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b019f9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\serializers.py\", line 458, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"d:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 692, in reducer_override\n",
      "    return self._function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 565, in _function_reduce\n",
      "    return self._dynamic_function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 546, in _dynamic_function_reduce\n",
      "    state = _function_getstate(func)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 157, in _function_getstate\n",
      "    f_globals_ref = _extract_code_globals(func.__code__)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle.py\", line 334, in _extract_code_globals\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                 ~~~~~^^^^^^^\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: IndexError: tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\serializers.py:458\u001b[39m, in \u001b[36mCloudPickleSerializer.dumps\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m pickle.PickleError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:73\u001b[39m, in \u001b[36mdumps\u001b[39m\u001b[34m(obj, protocol, buffer_callback)\u001b[39m\n\u001b[32m     70\u001b[39m cp = CloudPickler(\n\u001b[32m     71\u001b[39m     file, protocol=protocol, buffer_callback=buffer_callback\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43mcp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m file.getvalue()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:602\u001b[39m, in \u001b[36mCloudPickler.dump\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:692\u001b[39m, in \u001b[36mCloudPickler.reducer_override\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, types.FunctionType):\n\u001b[32m--> \u001b[39m\u001b[32m692\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    694\u001b[39m     \u001b[38;5;66;03m# fallback to save_global, including the Pickler's\u001b[39;00m\n\u001b[32m    695\u001b[39m     \u001b[38;5;66;03m# dispatch_table\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:565\u001b[39m, in \u001b[36mCloudPickler._function_reduce\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dynamic_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:546\u001b[39m, in \u001b[36mCloudPickler._dynamic_function_reduce\u001b[39m\u001b[34m(self, func)\u001b[39m\n\u001b[32m    545\u001b[39m newargs = \u001b[38;5;28mself\u001b[39m._function_getnewargs(func)\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m state = \u001b[43m_function_getstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (types.FunctionType, newargs, state, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    548\u001b[39m         _function_setstate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:157\u001b[39m, in \u001b[36m_function_getstate\u001b[39m\u001b[34m(func)\u001b[39m\n\u001b[32m    146\u001b[39m slotstate = {\n\u001b[32m    147\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__name__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__name__\u001b[39m,\n\u001b[32m    148\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__qualname__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__qualname__\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    154\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__closure__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__closure__\u001b[39m,\n\u001b[32m    155\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m f_globals_ref = \u001b[43m_extract_code_globals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__code__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m f_globals = {k: func.\u001b[34m__globals__\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f_globals_ref \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m\n\u001b[32m    159\u001b[39m              func.\u001b[34m__globals__\u001b[39m}\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle.py:334\u001b[39m, in \u001b[36m_extract_code_globals\u001b[39m\u001b[34m(co)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# We use a dict with None values instead of a set to get a\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[38;5;66;03m# deterministic order (assuming Python 3.6+) and avoid introducing\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[38;5;66;03m# non-deterministic pickle bytes as a results.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m out_names = {\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43moparg\u001b[49m\u001b[43m]\u001b[49m: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _, oparg \u001b[38;5;129;01min\u001b[39;00m _walk_global_ops(co)}\n\u001b[32m    336\u001b[39m \u001b[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[32m    337\u001b[39m \u001b[38;5;66;03m# syntax generates a constant code object corresponding to the one\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[38;5;66;03m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[38;5;66;03m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[32m    340\u001b[39m \u001b[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# add the result to code_globals\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: tuple index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPicklingError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdados2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\sql\\session.py:894\u001b[39m, in \u001b[36mSparkSession.createDataFrame\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m    889\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas.DataFrame):\n\u001b[32m    890\u001b[39m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[32m    891\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m).createDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[32m    892\u001b[39m         data, schema, samplingRatio, verifySchema\n\u001b[32m    893\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    896\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\sql\\session.py:938\u001b[39m, in \u001b[36mSparkSession._create_dataframe\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m    936\u001b[39m     rdd, struct = \u001b[38;5;28mself\u001b[39m._createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n\u001b[32m    937\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m938\u001b[39m jrdd = \u001b[38;5;28mself\u001b[39m._jvm.SerDeUtil.toJavaArray(\u001b[43mrdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_to_java_object_rdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    939\u001b[39m jdf = \u001b[38;5;28mself\u001b[39m._jsparkSession.applySchemaToPythonRDD(jrdd.rdd(), struct.json())\n\u001b[32m    940\u001b[39m df = DataFrame(jdf, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\rdd.py:3113\u001b[39m, in \u001b[36mRDD._to_java_object_rdd\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3110\u001b[39m rdd = \u001b[38;5;28mself\u001b[39m._pickled()\n\u001b[32m   3111\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3113\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx._jvm.SerDeUtil.pythonToJava(\u001b[43mrdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jrdd\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\rdd.py:3505\u001b[39m, in \u001b[36mPipelinedRDD._jrdd\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3502\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3503\u001b[39m     profiler = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3505\u001b[39m wrapped_func = \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3506\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\n\u001b[32m   3507\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3509\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3510\u001b[39m python_rdd = \u001b[38;5;28mself\u001b[39m.ctx._jvm.PythonRDD(\n\u001b[32m   3511\u001b[39m     \u001b[38;5;28mself\u001b[39m._prev_jrdd.rdd(), wrapped_func, \u001b[38;5;28mself\u001b[39m.preservesPartitioning, \u001b[38;5;28mself\u001b[39m.is_barrier\n\u001b[32m   3512\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\rdd.py:3362\u001b[39m, in \u001b[36m_wrap_function\u001b[39m\u001b[34m(sc, func, deserializer, serializer, profiler)\u001b[39m\n\u001b[32m   3360\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[33m\"\u001b[39m\u001b[33mserializer should not be empty\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3361\u001b[39m command = (func, profiler, deserializer, serializer)\n\u001b[32m-> \u001b[39m\u001b[32m3362\u001b[39m pickled_command, broadcast_vars, env, includes = \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3363\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sc._jvm.PythonFunction(\n\u001b[32m   3365\u001b[39m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[32m   3366\u001b[39m     env,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3371\u001b[39m     sc._javaAccumulator,\n\u001b[32m   3372\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\rdd.py:3345\u001b[39m, in \u001b[36m_prepare_for_python_RDD\u001b[39m\u001b[34m(sc, command)\u001b[39m\n\u001b[32m   3342\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prepare_for_python_RDD\u001b[39m(sc: \u001b[33m\"\u001b[39m\u001b[33mSparkContext\u001b[39m\u001b[33m\"\u001b[39m, command: Any) -> Tuple[\u001b[38;5;28mbytes\u001b[39m, Any, Any, Any]:\n\u001b[32m   3343\u001b[39m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[32m   3344\u001b[39m     ser = CloudPickleSerializer()\n\u001b[32m-> \u001b[39m\u001b[32m3345\u001b[39m     pickled_command = \u001b[43mser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3346\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) > sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[32m   3348\u001b[39m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\serializers.py:468\u001b[39m, in \u001b[36mCloudPickleSerializer.dumps\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    466\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (e.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, emsg)\n\u001b[32m    467\u001b[39m print_exec(sys.stderr)\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m pickle.PicklingError(msg)\n",
      "\u001b[31mPicklingError\u001b[39m: Could not serialize object: IndexError: tuple index out of range"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(dados2, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7eef20",
   "metadata": {},
   "source": [
    "# Usando a estrutura Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca604164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4185beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = [\n",
    "    Row(\"João\", 25),\n",
    "    Row(\"Maria\", 30),\n",
    "    Row(\"Pedro\", 26)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6c092f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\serializers.py\", line 458, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"d:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 692, in reducer_override\n",
      "    return self._function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 565, in _function_reduce\n",
      "    return self._dynamic_function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 546, in _dynamic_function_reduce\n",
      "    state = _function_getstate(func)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 157, in _function_getstate\n",
      "    f_globals_ref = _extract_code_globals(func.__code__)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle.py\", line 334, in _extract_code_globals\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                 ~~~~~^^^^^^^\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: IndexError: tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\serializers.py:458\u001b[39m, in \u001b[36mCloudPickleSerializer.dumps\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m pickle.PickleError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:73\u001b[39m, in \u001b[36mdumps\u001b[39m\u001b[34m(obj, protocol, buffer_callback)\u001b[39m\n\u001b[32m     70\u001b[39m cp = CloudPickler(\n\u001b[32m     71\u001b[39m     file, protocol=protocol, buffer_callback=buffer_callback\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43mcp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m file.getvalue()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:602\u001b[39m, in \u001b[36mCloudPickler.dump\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:692\u001b[39m, in \u001b[36mCloudPickler.reducer_override\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, types.FunctionType):\n\u001b[32m--> \u001b[39m\u001b[32m692\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    694\u001b[39m     \u001b[38;5;66;03m# fallback to save_global, including the Pickler's\u001b[39;00m\n\u001b[32m    695\u001b[39m     \u001b[38;5;66;03m# dispatch_table\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:565\u001b[39m, in \u001b[36mCloudPickler._function_reduce\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dynamic_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:546\u001b[39m, in \u001b[36mCloudPickler._dynamic_function_reduce\u001b[39m\u001b[34m(self, func)\u001b[39m\n\u001b[32m    545\u001b[39m newargs = \u001b[38;5;28mself\u001b[39m._function_getnewargs(func)\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m state = \u001b[43m_function_getstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (types.FunctionType, newargs, state, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    548\u001b[39m         _function_setstate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:157\u001b[39m, in \u001b[36m_function_getstate\u001b[39m\u001b[34m(func)\u001b[39m\n\u001b[32m    146\u001b[39m slotstate = {\n\u001b[32m    147\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__name__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__name__\u001b[39m,\n\u001b[32m    148\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__qualname__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__qualname__\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    154\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__closure__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__closure__\u001b[39m,\n\u001b[32m    155\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m f_globals_ref = \u001b[43m_extract_code_globals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__code__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m f_globals = {k: func.\u001b[34m__globals__\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f_globals_ref \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m\n\u001b[32m    159\u001b[39m              func.\u001b[34m__globals__\u001b[39m}\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle.py:334\u001b[39m, in \u001b[36m_extract_code_globals\u001b[39m\u001b[34m(co)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# We use a dict with None values instead of a set to get a\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[38;5;66;03m# deterministic order (assuming Python 3.6+) and avoid introducing\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[38;5;66;03m# non-deterministic pickle bytes as a results.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m out_names = {\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43moparg\u001b[49m\u001b[43m]\u001b[49m: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _, oparg \u001b[38;5;129;01min\u001b[39;00m _walk_global_ops(co)}\n\u001b[32m    336\u001b[39m \u001b[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[32m    337\u001b[39m \u001b[38;5;66;03m# syntax generates a constant code object corresponding to the one\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[38;5;66;03m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[38;5;66;03m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[32m    340\u001b[39m \u001b[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# add the result to code_globals\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: tuple index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPicklingError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\sql\\session.py:894\u001b[39m, in \u001b[36mSparkSession.createDataFrame\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m    889\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas.DataFrame):\n\u001b[32m    890\u001b[39m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[32m    891\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m).createDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[32m    892\u001b[39m         data, schema, samplingRatio, verifySchema\n\u001b[32m    893\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    896\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\sql\\session.py:938\u001b[39m, in \u001b[36mSparkSession._create_dataframe\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m    936\u001b[39m     rdd, struct = \u001b[38;5;28mself\u001b[39m._createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n\u001b[32m    937\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m938\u001b[39m jrdd = \u001b[38;5;28mself\u001b[39m._jvm.SerDeUtil.toJavaArray(\u001b[43mrdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_to_java_object_rdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    939\u001b[39m jdf = \u001b[38;5;28mself\u001b[39m._jsparkSession.applySchemaToPythonRDD(jrdd.rdd(), struct.json())\n\u001b[32m    940\u001b[39m df = DataFrame(jdf, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\rdd.py:3113\u001b[39m, in \u001b[36mRDD._to_java_object_rdd\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3110\u001b[39m rdd = \u001b[38;5;28mself\u001b[39m._pickled()\n\u001b[32m   3111\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3113\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx._jvm.SerDeUtil.pythonToJava(\u001b[43mrdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jrdd\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\rdd.py:3505\u001b[39m, in \u001b[36mPipelinedRDD._jrdd\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3502\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3503\u001b[39m     profiler = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3505\u001b[39m wrapped_func = \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3506\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\n\u001b[32m   3507\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3509\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3510\u001b[39m python_rdd = \u001b[38;5;28mself\u001b[39m.ctx._jvm.PythonRDD(\n\u001b[32m   3511\u001b[39m     \u001b[38;5;28mself\u001b[39m._prev_jrdd.rdd(), wrapped_func, \u001b[38;5;28mself\u001b[39m.preservesPartitioning, \u001b[38;5;28mself\u001b[39m.is_barrier\n\u001b[32m   3512\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\rdd.py:3362\u001b[39m, in \u001b[36m_wrap_function\u001b[39m\u001b[34m(sc, func, deserializer, serializer, profiler)\u001b[39m\n\u001b[32m   3360\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[33m\"\u001b[39m\u001b[33mserializer should not be empty\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3361\u001b[39m command = (func, profiler, deserializer, serializer)\n\u001b[32m-> \u001b[39m\u001b[32m3362\u001b[39m pickled_command, broadcast_vars, env, includes = \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3363\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sc._jvm.PythonFunction(\n\u001b[32m   3365\u001b[39m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[32m   3366\u001b[39m     env,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3371\u001b[39m     sc._javaAccumulator,\n\u001b[32m   3372\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\rdd.py:3345\u001b[39m, in \u001b[36m_prepare_for_python_RDD\u001b[39m\u001b[34m(sc, command)\u001b[39m\n\u001b[32m   3342\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prepare_for_python_RDD\u001b[39m(sc: \u001b[33m\"\u001b[39m\u001b[33mSparkContext\u001b[39m\u001b[33m\"\u001b[39m, command: Any) -> Tuple[\u001b[38;5;28mbytes\u001b[39m, Any, Any, Any]:\n\u001b[32m   3343\u001b[39m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[32m   3344\u001b[39m     ser = CloudPickleSerializer()\n\u001b[32m-> \u001b[39m\u001b[32m3345\u001b[39m     pickled_command = \u001b[43mser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3346\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) > sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[32m   3348\u001b[39m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AMBIENTE DE PROJETOS\\Anaconda_pySpark\\DATASETS\\.venv\\Lib\\site-packages\\pyspark\\serializers.py:468\u001b[39m, in \u001b[36mCloudPickleSerializer.dumps\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    466\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (e.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, emsg)\n\u001b[32m    467\u001b[39m print_exec(sys.stderr)\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m pickle.PicklingError(msg)\n",
      "\u001b[31mPicklingError\u001b[39m: Could not serialize object: IndexError: tuple index out of range"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449c11ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
